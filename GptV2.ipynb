{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812609 M parameters\n",
      "step 0: train loss 4.3354, val loss 4.3389\n",
      "step 100: train loss 2.5350, val loss 2.5419\n",
      "step 200: train loss 2.4192, val loss 2.4339\n",
      "step 300: train loss 2.3223, val loss 2.3338\n",
      "step 400: train loss 2.2280, val loss 2.2384\n",
      "step 500: train loss 2.1652, val loss 2.1917\n",
      "step 600: train loss 2.1055, val loss 2.1528\n",
      "step 700: train loss 2.0541, val loss 2.1098\n",
      "step 800: train loss 2.0154, val loss 2.0684\n",
      "step 900: train loss 1.9849, val loss 2.0519\n",
      "step 1000: train loss 1.9539, val loss 2.0439\n",
      "step 1100: train loss 1.9275, val loss 2.0045\n",
      "step 1200: train loss 1.8940, val loss 1.9939\n",
      "step 1300: train loss 1.8790, val loss 1.9667\n",
      "step 1400: train loss 1.8525, val loss 1.9508\n",
      "step 1500: train loss 1.8379, val loss 1.9617\n",
      "step 1600: train loss 1.8173, val loss 1.9429\n",
      "step 1700: train loss 1.7948, val loss 1.9314\n",
      "step 1800: train loss 1.7985, val loss 1.9301\n",
      "step 1900: train loss 1.7699, val loss 1.8895\n",
      "step 2000: train loss 1.7593, val loss 1.8950\n",
      "step 2100: train loss 1.7382, val loss 1.8885\n",
      "step 2200: train loss 1.7259, val loss 1.8723\n",
      "step 2300: train loss 1.7257, val loss 1.8589\n",
      "step 2400: train loss 1.7199, val loss 1.8645\n",
      "step 2500: train loss 1.7098, val loss 1.8587\n",
      "step 2600: train loss 1.7013, val loss 1.8674\n",
      "step 2700: train loss 1.6931, val loss 1.8408\n",
      "step 2800: train loss 1.6953, val loss 1.8359\n",
      "step 2900: train loss 1.6687, val loss 1.8375\n",
      "step 3000: train loss 1.6657, val loss 1.8170\n",
      "step 3100: train loss 1.6795, val loss 1.8309\n",
      "step 3200: train loss 1.6523, val loss 1.8295\n",
      "step 3300: train loss 1.6616, val loss 1.8216\n",
      "step 3400: train loss 1.6489, val loss 1.8171\n",
      "step 3500: train loss 1.6409, val loss 1.7997\n",
      "step 3600: train loss 1.6366, val loss 1.8263\n",
      "step 3700: train loss 1.6261, val loss 1.7919\n",
      "step 3800: train loss 1.6245, val loss 1.8043\n",
      "step 3900: train loss 1.6254, val loss 1.7921\n",
      "step 4000: train loss 1.6185, val loss 1.7799\n",
      "step 4100: train loss 1.6044, val loss 1.7761\n",
      "step 4200: train loss 1.6099, val loss 1.7869\n",
      "step 4300: train loss 1.5870, val loss 1.7657\n",
      "step 4400: train loss 1.6248, val loss 1.7740\n",
      "step 4500: train loss 1.6056, val loss 1.7600\n",
      "step 4600: train loss 1.5943, val loss 1.7573\n",
      "step 4700: train loss 1.5912, val loss 1.7719\n",
      "step 4800: train loss 1.5945, val loss 1.7645\n",
      "step 4900: train loss 1.5806, val loss 1.7479\n",
      "step 4999: train loss 1.5732, val loss 1.7395\n",
      "\n",
      "KING EDWARD IV:\n",
      "And braughte, again! how culthe's eyes,\n",
      "From be musty vulth. The reel beath!\n",
      " your bratus, caster's crient. Straved, for more, what iut;\n",
      "I the is too sague, and flire immelo man!\n",
      "Plect deseres see soe prupake\n",
      "Fillowhod a rooks; witho a vower's king mart\n",
      "Wher be that is donemy deedy's counts a royal hand for A terwion up: as the wine;\n",
      "I much to thou fluld: but do becate thou will be of they swearf de:\n",
      "To saight father, light, the fawilt'st us treven:\n",
      "Harry's in my willd arm forturt groyal me many,\n",
      "Hearth mindring;\n",
      "He's bean'd pour cousin'd laid frather\n",
      "On Mark worth, the shought desir,\n",
      "That, gaate yond--\n",
      "\n",
      "Have perings, and low\n",
      "The druit will not aursted Guiages?\n",
      "At what chy nather you, in such yieforth\n",
      "To to-day grave the true--\n",
      "Measures that: men kisgo the doth youriff, lead thee: there deperniace moother?\n",
      "Am;, shall sir, that his degrad so.\n",
      "Of me the leaes bother you are, as anginte shep\n",
      "As he propens of sendehard? my hut warraw you,\n",
      "Ard skikence not. So\n",
      "horter's nothernmer are we seat knee--\n",
      "Who Werlf, this when day.\n",
      "\n",
      "MONTAGULO:\n",
      "Come, by now the prove\n",
      "Lorship-desurest his comentied:\n",
      "And I be let.\n",
      "Some her such at--so?\n",
      "\n",
      "MARCIUS:\n",
      "We'll the south!\n",
      "Or weary unthese,---hear saw, bearl,\n",
      "Answe not ke hit king. Here you, could me\n",
      "And sry that southty:\n",
      "He fishard; genot mistres, a min made!\n",
      "I what short, icked the bein thy grackes adure\n",
      "I can me carrocken the guilty.\n",
      "Go, and come, in the kill racks, pothen,\n",
      "Which ha to conder any daster, she\n",
      "Bears me nother browb and her father\n",
      "And conwe-morne, were shcontempated\n",
      "The borought, and servand I crit't should\n",
      "be he sward by the nears, were ofglish\n",
      "And vake leete mine corrihels sight; I dabb'st, we conself to ever the frier.\n",
      "\n",
      "BUCK:\n",
      "And minky-hearth.\n",
      "\n",
      "MENo: but therefore thou art torn Varridy's thus not to-do I\n",
      "hapted thy behody cheep of aloved your\n",
      "nemp\n",
      "To disperitorsed that shook'd help kings my spire:\n",
      "Disterit Waymery.\n",
      "God one her mark?\n",
      "Marchorthy newss cond to kill ne slaid enfits.\n",
      "The rue! Onsper aftire off, \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128  # Aumentado de 64 para 128\n",
    "n_head = 8    # Aumentado de 4 para 8\n",
    "n_layer = 4\n",
    "dropout = 0.1 # Adicionado Dropout para regularização\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
